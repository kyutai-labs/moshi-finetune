{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyuOCYM92LJb"
   },
   "source": [
    "# Getting Started with Fine-Tuning Moshi 7B\n",
    "\n",
    "This notebook shows you a simple example of how to LoRA finetune Moshi 7B. You can run this notebook in Google Colab using a A100 GPU.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github//kyutai-labs/moshi-finetune/blob/main/tutorials/moshi_finetune.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Check out `moshi-finetune` Github repo to learn more: https://github.com/kyutai-labs/moshi-finetune/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxr8mv-17GfB"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Clone the `moshi-finetune` repo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TIj3IlIeVDIb",
    "outputId": "6ffd6946-26a2-4e3d-e6db-3336cc2c7444"
   },
   "outputs": [],
   "source": [
    "%cd /content/\n",
    "!git clone https://github.com/kyutai-labs/moshi-finetune.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQPd_pGT7WiY"
   },
   "source": [
    "Install all required dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KuTOGipl7BS7",
    "outputId": "0d332b99-54b1-431b-eb41-4b929087040c"
   },
   "outputs": [],
   "source": [
    "%pip install -e /content/moshi-finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ams-19wF8zgY"
   },
   "source": [
    "## Prepare dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "Path(\"/content/data/daily-talk-contiguous\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the dataset\n",
    "local_dir = snapshot_download(\n",
    "    \"kyutai/DailyTalkContiguous\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=\"/content/data/daily-talk-contiguous\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hia7n0T1_mHZ"
   },
   "source": [
    "## Start training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtcLerooWFeB"
   },
   "outputs": [],
   "source": [
    "# these info is needed for training\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dxTlIQMaJGv"
   },
   "outputs": [],
   "source": [
    "# define training configuration\n",
    "# for your own use cases, you might want to change the data paths, model path, run_dir, and other hyperparameters\n",
    "import yaml\n",
    "\n",
    "config = \"\"\"\n",
    "# data\n",
    "data:\n",
    "  train_data: '/content/data/daily-talk-contiguous/dailytalk.jsonl' # Fill\n",
    "  eval_data: '' # Optionally Fill\n",
    "  shuffle: true\n",
    "\n",
    "# model\n",
    "moshi_paths:\n",
    "  hf_repo_id: \"kyutai/moshiko-pytorch-bf16\"\n",
    "\n",
    "\n",
    "full_finetuning: false # Activate lora.enable if partial finetuning\n",
    "lora:\n",
    "  enable: true\n",
    "  rank: 128\n",
    "  scaling: 2.\n",
    "  ft_embed: false\n",
    "\n",
    "# training hyperparameters\n",
    "first_codebook_weight_multiplier: 100.\n",
    "text_padding_weight: .5\n",
    "\n",
    "\n",
    "# tokens per training steps = batch_size x num_GPUs x duration_sec\n",
    "# we recommend a sequence duration of 300 seconds\n",
    "# If you run into memory error, you can try reduce the sequence length\n",
    "duration_sec: 100\n",
    "batch_size: 1\n",
    "max_steps: 300\n",
    "\n",
    "gradient_checkpointing: true # Activate checkpointing of layers\n",
    "\n",
    "# optim\n",
    "optim:\n",
    "  lr: 2.e-6\n",
    "  weight_decay: 0.1\n",
    "  pct_start: 0.05\n",
    "\n",
    "# other\n",
    "seed: 0\n",
    "log_freq: 10\n",
    "eval_freq: 1\n",
    "do_eval: False\n",
    "ckpt_freq: 10\n",
    "\n",
    "save_adapters: True\n",
    "\n",
    "run_dir: \"/content/test\"  # Fill\n",
    "\"\"\"\n",
    "\n",
    "# save the same file locally into the example.yaml file\n",
    "with open(\"/content/example.yaml\", \"w\") as file:\n",
    "    yaml.dump(yaml.safe_load(config), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErD1ktQUMyPZ"
   },
   "outputs": [],
   "source": [
    "# make sure the run_dir has not been created before\n",
    "# only run this when you ran torchrun previously and created the /content/test_ultra file\n",
    "# ! rm -r /content/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4wFgmwIUTtg",
    "outputId": "8fe22185-6e12-4987-c4f6-3768952cec7c"
   },
   "outputs": [],
   "source": [
    "# start training\n",
    "\n",
    "!cd /content/moshi-finetune && torchrun --nproc-per-node 1 -m train /content/example.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruJ29JFn98zE"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Once the model has been trained, inference can be run on the colab GPU too, and gradio can be used to tunnel the audio data from a local client to the notebook.\n",
    "\n",
    "More details on how to set this up can be found in the [moshi readme](https://github.com/kyutai-labs/moshi?tab=readme-ov-file#python-pytorch).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-xLs2Ot9-il",
    "outputId": "f0c6f171-b14c-4d0c-d5e9-cb24a7f07653"
   },
   "outputs": [],
   "source": [
    "!python -m moshi.server --gradio-tunnel --lora-weight=/content/test/checkpoints/checkpoint_000300/consolidated/lora.safetensors --config-path=/content/test/checkpoints/checkpoint_000300/consolidated/config.json"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
